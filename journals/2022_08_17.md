- [Harmony Search Method: Theory and Applications](https://www.hindawi.com/journals/cin/2015/258491/) #[[Harmony Search]]
	- ## Algorithm
		- 1. Initialize the HS Memory (HM), which consists of a certain number of randomly generated solutions to the optimization problems under consideration. For an $n$-dimension problem, an HM with the size of $N$ can be represented as follows:
		  $$
		  HM = 
		  \begin{bmatrix}
		  x_1^1 & x_2^1 & \dots & x_n^1 \\
		  x_1^2 & x_2^2 & \dots & x_n^2 \\
		  \vdots  & \vdots & \ddots & \vdots\\
		  x_1^N & x_2^N & \dots & x_n^N \\
		  \end{bmatrix}
		  $$
		  2. Improvise a new solution $[x_1', x_2', \dots, x_n']$ from the HM. Using Harmony Memory Considering Rate (HMCR) as the probability of selecting a component from the HM members, and Pitching Adjust Rate (PAR) as the probability to mutate a candidate from the HM.
		  3. Update the HM. The new solution from Step 2 is evaluated. If it yields a better fitness than that of the worst member in the HM, it will replace that one. Otherwise, it is eliminated.
		  4. Repeat Step 2 to Step 3 until a preset termination criterion is met.
	- ## Variants
		- [[GHS]]
			- Changes the adjustment of new solutions to be only based on the best harmony selected from the HM without the involvement of the distance bandwidth (bw).
			- Adds the unique social learning capability to the GHS
		- [[DLHS]] is a local-best variant of the [[Harmony Search]] method with dynamic subpopulation.
			- The whole HM is divided into multiple sub-HMs, each of which can evolve independently. However, these sub-HMs will form the HM again after searching fo the optimal solutions in their own regions.
- How is this different from [[evolution strategies]]? #Questions
-